{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier\n",
    "A classifier takes a vector as input and returns a class label. In our scenario, we only have binary classes (each of the attributes is either 0 or 1).\n",
    "\n",
    "## Logistic Regression\n",
    "More info about the logistic regression classifier can be found [here](https://towardsdatascience.com/the-perfect-recipe-for-classification-using-logistic-regression-f8648e267592) or [here](https://builtin.com/machine-learning/logistic-regression-classification-algorithm). Here is a short description of the main idea. Each of the vectors can be represented as data points in an n-dimensional space. Linear Regressions tries to find a line/hyperplane in function of your x, where f(x) gives your y value. You search this line during training, where you try to find the line/hyperplane that minimizes this (least squares) error. Logistic Regression does something similar, but to classify binary values. For this, it tries to fit a logistic function instead of a linear function (using maximum likelihood).\n",
    "\n",
    "## LinearSVC\n",
    "More info can be found [here](https://towardsdatascience.com/the-perfect-recipe-for-classification-using-logistic-regression-f8648e267592) or [here](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer). Support vector classifiers try to find the best soft margin (based on validation set) which acts as a seperator line/hyperplane between the different classes. Support vector machines transfer the data points to a higher dimensional space, where it might be easier to find a seperator line (using support vector classifiers. The kernel function is used to transform the data points to a higher dimensional space. ([src](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer))\n",
    "\n",
    "# Data imbalance explanation\n",
    "When the data is imbalanced, the classifier can be biased towards one class. This is because the classifier is trying to fit a line/hyperplane that minimizes the error (least squares). This problem can often be solved by trying to rebalance the dataset. This can be done e.g. by oversampling, where you add more points of the minority class (e.g. by resampling). Undersampling is also possible. This is done by removing points of the majority class. There are also other methods that combine oversampling and undersampling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}