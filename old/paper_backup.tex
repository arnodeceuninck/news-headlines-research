\documentclass{article}

\usepackage[english]{babel}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% allow subsubsubsections using \paragraph{}
% \usepackage{titlesec}
% \setcounter{secnumdepth}{4}

\title{Winner prediction for Dutch news headlines}
\author{Arno Deceuninck}

\begin{document}
\maketitle

\section{Introduction}
\subsection{Previous research}
Soubry \cite{soubry} analysed a set of over 2000 potential Dutch news headlines for over 900  articles. Each of those articles has at least two candidate headlines of which only one got labeled as "winner" using AB tests. In her research, she manually analyzed different charecteristics of all those potential headlines - activity, length, questions, punctuation, duality, emotions, forward reference, signal words, article words, adjectives, proper names, directly adressing the reader, first and last names, numbers and quotations. \cite{soubry}

\subsection{This research}
This project tries to automatically predict those winners given a set of candidate news headlines, which might be useful for recommending journalists which of their candidate headlines has the highest probability for getting clicked on. To do this, I first looked at Natural Language Processing techniques (NLP) to find the manually labeled characteristics automatically, looked at potential extra features and tried different (classification) methods to find the headline that would generate the most clicks.

\section{Label classification}
The first step is automatically detecting the characteristics Soubry \cite{soubry} labeled. To test the different models, a train-test split was generated by first sampling one headline per article \footnote{I started by generating the test set using all candidate headlines (without dropping the ones that weren't sampled), since the labels should be independent of the other headlines in the test set. However, headlines in the same test set often look very similar and thus share the same values for a lot of the features, which lead to a train and test set that weren't independent from each other (and thus also the impression of giving a better performance, even though this wasn't the case).} and then taking 25\% of this as test set and 75\% as training set.

\subsection{Concepts}
Some NLP concepts are the same for a lot of those classification tasks, so I'll explain them first.

\subsubsection{Preprocessing}
The text is often preprocessed e.g. by removing special characters, removing stopwords and/or making everything lowercase.

\subsubsection{Tokenizer}
Once we have our preprocessed text, we can tokenize it. A token is a basic unit that won't get decomposed any further. You can e.g. take words or characters as token or combine multiple consequent words or characters into compound tokens (n-grams). \cite{webster1992tokenization}

\paragraph{Stemming}
Stemming maps words containing the same stem (e.g. "unicorn" and "unicorns", "creation" and "create") onto the same token (e.g. "unicorn-", "creat-").  \cite{patel2021fake}

\subsubsection{Vectorizer}
The vectorizer converts the text to a meaningful numerical representation that enables the machines to understand the textual contents. \cite{singh2019vectorization} This numerical vector representation can then be used on e.g. already known classification vectors that take a numerical vector as input.


\paragraph{Bag of Words}
Bag of Words (also called CountVectorizer) genertes a vector containing the number of occurances in the input text of the word given position in the vector represents. \cite{patel2021fake}

\paragraph{TF-IDF}
The Term Frequency-Inverse Document Frequency (TF-IDF) is similar to Bag of Words, but also the number of occurances of given words in the entire document collection into account (so often occuring words like "the" become less important). \cite{patel2021fake}

\subsubsection{Extra features}
Sometimes some extra features where added to the input vector for the classifier (together with the output of the vectorizer). Often those extra features were based on NLP libraries like SpaCy and NLTK \footnote{Mainly on SpaCy, since the performance of NLTK on Dutch headlines was quite bad.}, which extract features of the text like part-of-speech tags, named entities or dependency parse trees. \cite{vasiliev2020natural}

\subsubsection{Classifier}
The classifiers take a vector of numbers as input and outputs a class label. I mainly used a SVM classifier here, but other options might give better results\footnote{I used the SVM classifier because it occured in one of the first text classification tutorials I found. However, you can keep endlessly searching for better classification methods and since this was only a small part of the goal for this project, I didn't test other classifiers.}. %\cite{sklearn_classification_2022}

\paragraph{SVM classifier}
 Each of the n-dimensional input vectors can be seen as a point in an n-dimensional space. Support vector machines (SVM) try to find the best soft margin (based on validation set) which acts as a seperator line/hyperplane between the different classes. They transfer the data points to a higher dimensional space, where it might be easier to find a seperator line. The kernel function is used to transform the data points to a higher dimensional space. In my implementation, only a linear kernel is used\footnote{Other kernels might give better results, but you have to see how far you want to keep searching, since the search for the optimal classifier might be almost endless.}. \cite{gholami2017support}

\subsubsection{Simple functions}
Sometimes just using a simple functions (e.g. whether the headline contains a question mark) gave better results than a more complex method. However, we have to be carefull for overfitting with this.

\subsubsection{Metrics}
An evaluation metric tries to measure the quality of a classification model. Since we're working with binary classifications (positive or negative), we have four possible classification outputs. If the data is correctly classified, we have a True Positive (TP) or True Negative (TN). If the prediction is wrong, we have a False Negative (when it should be positive, but got classified as negative, FP) or False Positive (when it should be negative, but got classified as positive). Those four values can be found in the confusion matrix. \cite{agarwal_2019}

\paragraph{Accuracy}
The accuracy is the proportion of correct classification among the total number of classified items. \cite{agarwal_2019}

\paragraph{Data imbalance} If the data is imbalanced, accuracy will give missleading results. E.g. if 99\% of our labeled items is negative, we can achieve an accuracy of 99\% by classifying everything as negative. The f1-score is an evalution metric that solves this data imbalance problem.

We'll mainly focus on the f1-score when evaluating the characteristics detection.

\subsubsection{Data imbalance}
Sometimes, the data might be imbalanced, meaning that we have a lot more samples from one of the classes than from the other. You can try to solve this by adding extra samples from the minority class (oversampling), removing samples from the majority class (undersampling) or a combination of both.

\subsection{Characteristics}
We'll now take a look at the different labels in the dataset from Soubry \cite{soubry} and try to predict those labels automatically.

\subsubsection{Activity}
This label represents whether the sentence was writen in active form. If this is false, the sentence is written in passive form. You can find the results in table \ref{tab:activity}.

\paragraph{Custom functions}
I initially tried to detect passive sentences using a simple function that tries to find past particles (only checks for words starting with "ge-" (e.g. "gevochten")), the word "door" or forms of "zijn" or "worden". Here is the earlier mentioned danger of overfitting applicable. Since a lot of words starting with "ge-" aren't a past particle, I also tried only checking on the word "door" or forms of "zijn" or "worden", which worked a lot better.

For the next approach, I used SpaCy to detect whether any of the words in the headlines is a past particle. If that was the case, it got classified as a passive sentence. This also worked slightly better than my own functions.

\paragraph{TF-IDF words} Applying TF-IDF using words as tokens gave the best results. I also did an analysis of the most important words in this model. With this, I expected a lot of past particles as words on the negative (passive) side, but that wasn't the case as much as I expected. The top 5 most important words for classifying something as passive are "opgepakt", "twee", "dj", "omdat" and "geschiedenis" accoording to this model. "Opgepakt" is the only past particle of those words. \\
The top 5 most important words for classying something as active are "geeft", "onze", "vlaanderen", "politie" and "auto". "Geeft" is the only one I would have expected here, since it's hard to put that word in a passive sentence.

\paragraph{TF-IDF chars} Another approach was by using character grams of length one to four. I thougth this would generalize better to detect past particles by looking at char grams (taking white spaces into account) like " ge", "d ", " ver" or "door". This performed similar to TF-IDF for words (slightly worse). The top 5 char grams for classifying something as pasive were " ak", " era", "ng ", "erd" and "  d". Some of those indeed occur often in a past particle (like "erd"), but I don't see why e.g. " ak" has the largest impact.

\paragraph{Combination} I also tried combining the TF-IDF methods with an extra feature whether the headline contains a past particle accoording to SpaCy. I wouldn't expect this to improve for the word based TF-IDF, since this can already detect past particles using the entire words, but the character based TF-IDF couldn't do this, so I expected a better performance there. There was indeed a slight increase in performance for the character based TF-IDF (but nothing significant), but strangely also a small decrease in the word based TF-IDF (but also nothing significant).


\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                          & \textbf{F1-score} & \textbf{Accuracy} \\
Simple function (ge-, door, zijn/worden) & 58.42\%           & 46.93\% (107/228) \\
Simple function (door, zijn/worden)      & 77.14\%           & 64.91\% (148/228) \\
Past particle SpaCy                      & 79.42\%           & 68.86\% (157/228) \\
TF-IDF (words)                           & 91.04\%           & 83.70\% (190/227) \\
TF-IDF (chars (1-4))                     & 90.29\%           & 82.38\% (187/227) \\
Combination (SpaCy + TF-IDF chars)       & 90.51\%           & 82.82\% (188/227) \\
Combination (SpaCy + TF-IDF words)       & 90.78\%           & 83.26\% (189/227)
\end{tabular}
\caption{Detecting label "Actief"}
\label{tab:activity}
\end{table}

\subsubsection{Length}
Longer headlines can give more information to the reader. Soubry defined as long if it contains 76 or more characters. \cite{soubry} You can find the results in table \ref{tab:length}.

\paragraph{Simple function} The definition is clear, so I could just write a function for finding all the exact matches, of which you would expect a perfect accuracy. However, there were still a few misclassifications, which were wrongly labeled in the dataset of Soubry.

\paragraph{Length as feature} Since we didn't get a perfect match with the previous method, I also tried giving the length as only feature to a logistic regression classifier (so it could find it's own optimal border length). I would've expected slightly better results with this, but it was actually slightly worse than the previous function, probably because it is overfitting slightly on the training set.


\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                          & \textbf{F1-score} & \textbf{Accuracy} \\
Simple function (length $\geq$ 76) & 98.34\%           & 97.81\% (223/228) \\
Logistic regression (length as feature)      & 97.95\%           & 97.37\% (222/228)
\end{tabular}
\caption{Detecting label "Lang"}
\label{tab:length}
\end{table}

\subsubsection{Questions}
Question should generate more clicks, since it makes people curious about the answer. \cite{soubry} You can find the results in table \ref{tab:questions}.

\paragraph{Simple function} A questions usually comes with a question mark, so just checking for this might already give good results. This still (relatively) gave quite some false positives. When checking what the false positives in the training set were, it gave mainly retorical questions, questions in quotations or question that already contain the answer in the headline itself.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                          & \textbf{F1-score} & \textbf{Accuracy} \\
Simple function (contains question mark) & 78.95\%           & 96.48\% (219/227)
\end{tabular}
\caption{Detecting label "Vragen"}
\label{tab:questions}
\end{table}

\subsubsection{Punctuation}
Sourby mentions three kinds of punctuation: exclamation marks (!), ellipses (...) and quotation marks. You can find the results in table \ref{tab:punctuation}.

\paragraph{Simple function} Since this contains a clear definition, it can be easily checked using a function that checks for any of the three mentioned punctuation methods. I expected a perfect result with this, but that was not the case. It was also unclear why some classifications were wrong. My guess is that this is because of misclassifications in the original dataset or that there's an extra condition to be labeled for punctuation which I don't see.

\begin{table}
\begin{tabular}{lll}
\textbf{Method}                          & \textbf{F1-score} & \textbf{Accuracy} \\
Simple function (contains !, ... or '') & 57.14\%           & 92.07\% (209/227)
\end{tabular}
\caption{Detecting label "Interpunctie"}
\label{tab:punctuation}
\end{table}

\subsubsection{Duality}
Some headlines consists of two parts, like a headline as first part and a quote as second part. Those two parts are often seperated by a double points or a point. \cite{soubry} You can find the results in table \ref{tab:duality}.

\paragraph{Simple function} This also contains a clear definition, so I'll just check whether the headline contains a "." or a ":".

\begin{table}
\begin{tabular}{lll}
\textbf{Method}                          & \textbf{F1-score} & \textbf{Accuracy} \\
Simple function (contains "." or ":") & 90.52\%           & 90.31\% (205/227)
\end{tabular}
\caption{Detecting label "Tweeledigheid"}
\label{tab:duality}
\end{table}

\subsubsection{Emotions}
Some headlines contain emotions to trigger te reader to find out more about the entire story. \cite{soubry} You can find the results in table \ref{tab:emotions}.

\paragraph{EmoRoBERTa} EmoRoBERTa is a model for emotion detection on English sentences. It doesn't just detect wheter a sentence contains an emotion, but also assigns the detected emotions with their corresponding probabilities to the sentences. However, this won't work on Dutch sentences of course. I tried to translate the sentences to English and then feed them to EmoRoBERTa. This was able to correctly classify the emotions itself, but less good at predicting our binary emotion label (since Soubry e.g. didn't see "curiosity" as an emotion, but adding those more neutral emotions also didn't fix this). Translating the sentences is also a bit cheating, so we won't use it anymore.

\paragraph{TF-IDF} TF-IDF might also work good, since it just needs to learn the word that have an emotional connotation. However, this predicted everything as negative. Adding stemming slightly improved this, but we still have a larger problem of data imbalance.

\paragraph{Data imbalance} In our trining dataset, only 69 of the 681 headlines contain an emotion, which makes the classes of containing or not containing an emotion highly unbalanced. I tried some different approaches like reducing the number of features, different methods for oversampling, different methods for undersampling and the combination oversampling and undersampling. However, the f-score stayed quite low.

\paragraph{BERTje} BERTje is the Dutch version of BERT, which is a transformer based machine learning technique. \cite{de2019bertje} I tried finetuning this on our binary emotion classification task. It took quite some time to train to result in slightly worse results than just using TF-IDF and it also had a lower f-score.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                                             & \textbf{F1-score} & \textbf{Accuracy} \\
EmoRoBERTa                                                  & 35.29\%           & 75.77\% (172/227) \\
TF-IDF                                                      & 00.00\%           & 87.28\% (199/228) \\
TF-IDF (stem)                                               & 33.33\%           & 89.47\% (204/228) \\
TF-IDF (stem + low max features)                            & 00.00\%           & 87.28\% (199/228) \\
TF-IDF (stem + high max features)                           & 33.33\%           & 89.47\% (204/228) \\
TF-IDF (stem + oversampling (random))                       & 35.90\%           & 89.04\% (203/228) \\
TF-IDF (stem + oversampling (SMOTE))                        & 35.90\%           & 89.04\% (203/228) \\
TF-IDF (stem + oversampling (Borderline SMOTE))             & 35.90\%           & 89.04\% (203/228) \\
TF-IDF (stem + oversampling (ADASYN))                       & 35.90\%           & 89.04\% (203/228) \\
TF-IDF (stem + undersampling (random))                      & 41.41\%           & 71.49\% (163/228) \\
TF-IDF (stem + undersampling (NearMiss v2))                 & 37.50\%           & 64.91\% (148/228) \\
TF-IDF (stem + undersampling (Condensed Nearest Neighbors)) & 40.91\%           & 88.60\% (202/228) \\
TF-IDF (stem + undersampling (TomekLinks))                  & 37.84\%           & 89.91\% (205/228) \\
TF-IDF (stem + resampling (SMOTEENN))                       & 22.57\%           & 12.72\% (29/228)  \\
TF-IDF (stem + resampling (SMOTETomek))                     & 35.90\%           & 89.04\% (203/228) \\
BERTje                                                      & 27.03\%           & 88.16\% (201/228)
\end{tabular}
\caption{Detecting label "Emotie"}
\label{tab:emotions}
\end{table}

\subsubsection{Forward Reference}
Forward references keep a part of the article unknown for the reader, until they click it open. It's often in the form of words like "this", "he"/"she", "why", "who" ... . Also not further specified headlines get seen as forward references. \cite{soubry} I tried to search some papers about detecting clickbait (since this also often contains forward references), but without much succes. You can find the result of the TF-IDF approach in tble \ref{tab:forward}

\paragraph{TF-IDF} TF-IDF (with SMOTTomek as resampling method and stemming) still gave quite a bad f-score. I would've expected better performance with this, since there are often the same forward referencing words. The most important stems where indeed "dez-", "dit", "zo", "waarom" ... which is what you would expect in forward references.

\begin{table}
\begin{tabular}{lll}
\textbf{Method}                          & \textbf{F1-score} & \textbf{Accuracy} \\
TF-IDF (SMOTETomek + stem) & 47.73\%           & 79.47\% (181/227)
\end{tabular}
\caption{Detecting label "Voorwaartse Verwijzing"}
\label{tab:forward}
\end{table}

\subsubsection{Signal words}
Some headlines contain signal words like "LIVE" or "SPORT". \cite{soubry} You can find the results in table \ref{tab:signal}.
'
\paragraph{Simple function} Those signal words are often full caps, so I started by classifying all headlines containing a full caps word as positive.

\paragraph{TF-IDF} Since the signal words are often the same occuring words, I would expect good performance here. The accuracy is indeed higher than the previous method, but the f-score is lower.

\paragraph{Extra features} Along TF-IDF, I also added three extra features: whether there is a dot after the first word, whether the first word is full caps and whether it contains any full caps word. This resulted in a better accuracy and f-score.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                                              & \textbf{F1-score} & \textbf{Accuracy} \\
Simple Function (any full caps)                              & 53.85\%           & 94.74\% (216/228) \\
TF-IDF                                                       & 40.00\%           & 97.37\% (222/228) \\
Extra Features (TF-IDF, dot, first full caps, any full caps) & 85.71\%           & 99.12\% (226/228)
\end{tabular}
\caption{Detecting label "Signaalwoorden"}
\label{tab:signal}
\end{table}

\subsubsection{Article words}
Articles are often omitted from news headlines. In this analyses, they only looked at articles that can be omitted and not articls in fixed expressions (e.g. "uit de biecht klappen", "Bart De Pauw"). \cite{soubry} Just detecting articles is easy, the hard part is detecting whether an article could be omitted. You can find the results in table \ref{tab:articles}.

\paragraph{Simple function}
Just detecting whether there is some article ("de", "het", "een") in the headline might already give quite some good results, though they were still lower than I initially would've expected. I also tried to do the same, but after removing text between quotation marks (since that counts as fixed expressions). That gave as expected an increase in accuracy and f-score.

\paragraph{TF-IDF}
Since it just needs to detect articles, I would expect TF-IDF to give a similar performance as my first simple function. The accuracy was slightly better but the f-score was slightly worse. The articles still got seen as the most important words in this model. I also tried TF-IDF on multiple words, since this might be better to detect fixed expression. However, there weren't any expressions in the most important word pairs. I also tried here first removing the quotations from the headline and then applying TF-IDF.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                       & \textbf{F1-score} & \textbf{Accuracy} \\
Simple Function (articles)            & 58.99\%           & 75.00\% (171/228) \\
Simple Function (articles, no quotes) & 66.13\%           & 81.58\% (186/228) \\
TF-IDF                                & 45.07\%           & 82.89\% (189/228) \\
TF-IDF (1-2 words)                    & 49.38\%           & 82.02\% (187/228) \\
TF-IDF (1-3 words)                    & 45.78\%           & 80.26\% (183/228) \\
TF-IDF (no quotes, 1-2 words)         & 62.34\%           & 87.28\% (199/228)
\end{tabular}
\caption{Detecting label "Lidwoorden"}
\label{tab:articles}
\end{table}

\subsubsection{Adjectives}
Adjectives give extra information and are often used to add extra emotions. The hard part here is that not all adjectives count, only important ones that have a significant meaning and difference from the other candidate headlines. \cite{soubry} You can find the results in table \ref{tab:adjectives}.

\paragraph{TF-IDF} We just need to know what adjectives are, so using Bag of Words or TF-IDF might already give quite good results. The results where lower than expected, certainly the f-score.

\paragraph{Simple function} SpaCy can also recognize wheter a given word in a sentence is an adjective, so we can just check this for any word.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}            & \textbf{F1-score} & \textbf{Accuracy} \\
Bag of Words (SMOTETomek)  & 37.70\%           & 66.67\% (152/228) \\
TF-IDF (SMOTETomek)        & 17.65\%           & 75.44\% (172/228) \\
TF-IDF (SMOTETomek + stem) & 18.92\%           & 73.68\% (168/228) \\
SpaCy (contains adjective) & 44.54\%           & 44.30\% (101/228)
\end{tabular}\caption{Detecting label "Adjectieven"}
\label{tab:adjectives}
\end{table}

\subsubsection{Proper names}
Proper names might give the reader a better connection to the article. \cite{soubry} You can find the results in \ref{tab:proper}.

\paragraph{TF-IDF} Since the same proper names occur often in the news, TF-IDF might do quite a good job, but also here is the f-score lower than expected. Here is also the danger of overfitting  on names that were relevant at the time the data got collected and new names won't be detected. The most important fatures are "Kevin", "Temptation" (Island) and "Blind" (Getrouwd), but "zijn" and "tegenslag" make less sense.

\paragraph{Simple function} Spacy can appearantly also detect proper nouns. However, this results in a very low accuracy. Some of them might be labeld wrongly in the dataset (e.g. "Gert" or "Verhofstadt"). SpaCy also marks words like "Manneke Pis" as proper nouns, but they aren't in the dataset.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}            & \textbf{F1-score} & \textbf{Accuracy} \\
TF-IDF (SMOTETomek)        & 29.41\%           & 89.47\% (204/228) \\
SpaCy (contains proper noun) & 25.43\%           & 43.42\% (99/228)
\end{tabular}\caption{Detecting label "Eigennamen"}
\label{tab:proper}
\end{table}

\subsubsection{Adressing the reader}
Some headlines directly adress the reader, like "How can I sleep in this warm weather?" or "How to get rid of pimples?". \cite{soubry}. You can find the results in table \ref{tab:adress}.

\paragraph{TF-IDF} TF-IDF might recognize some words that get often used in those sentences. The most important positive words are as expected "je", "waarom" and "bent". Adding stemming didn't make a difference here.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}            & \textbf{F1-score} & \textbf{Accuracy} \\
TF-IDF (SMOTETomek)        & 29.41\%           & 89.47\% (204/228) \\
SpaCy (contains proper noun) & 25.43\%           & 43.42\% (99/228)
\end{tabular}\caption{Detecting label "Betrekking"}
\label{tab:adress}
\end{table}

\subsubsection{First and Last name}
This label checks whether both the first and last name is used instead of only one of the two. \cite{soubry} You can find the results in table \ref{tab:firstlast}.

\paragraph{TF-IDF} This learns the names of the people that were in the news when the data was collected. When using only one word tokens, most names have a positive impact, except for Trump, what makes sense, since there is often news about him, so most people already know him, so they almost never say Donald Trumps. Only allowing bi- or trigrams makes more sense to detect the entire name, however this strangely leads to a worse performance.

\paragraph{Simple Function} SpaCy can detect proper nouns. To find a first and last name combination, we can simply check for two consecutive proper nouns. This resulted in a better f-score, but a lower accuracy. Things like "Mnneke Pis", "Club Brugge" and "Stille Ocean" remain false positive, but they might be easily filtered out by still adding TF-IDF as extra feature. Adding this, makes it indeed better, but strangely it is still better with unigrams than with bigrams.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                                                    & \textbf{F1-score} & \textbf{Accuracy} \\
TF-IDF                                                             & 19.05\%           & 92.54\% (211/228) \\
TF-IDF (2-3 words)                                                 & 9.52\%            & 91.67\% (209/228) \\
SpaCy (consecutive proper nouns)                                   & 52.46\%           & 87.28\% (199/228) \\
Combination (TF-IDF (1 word) + SpaCy  (consecutive proper nouns))  & 66.67\%           & 93.86\% (214/228) \\
Combination (TF-IDF (2 words) + SpaCy  (consecutive proper nouns)) & 57.14\%           & 90.79\% (207/228)
\end{tabular}
\caption{Detecting label "Voor+Achternaam"}
\label{tab:firstlast}
\end{table}

\subsubsection{Numbers}
Numbers make the content of the article predictable. It can make the headlines more informative or add more emotion. \cite{soubry} You can find the results in table \ref{tab:numbers}.

\paragraph{TF-IDF} If there are enough numbers in the training set, TF-IDF might already give quite good results with words. I would've expected different numbers as most important words, but that's not really the case. The top 5 is "jaar", "000", "euro", "graden" and "24". Things like "jaar", "euro" and "graden" all make sense and is a more out of the box approach. Since most numbers consist of the same characters (0-9) or contain the same textual parts (e.g. "zes", "-tig" in Dutch), working with char-grams might also give quite good results. The most important char grams were as expected mainly digits, but also brackets (since e.g. the age of people is often put inside brackets).

\paragraph{Simple function} I also tried an own function that checks whether the headline contains a digit or contains a word with one of a small list of numbers as part of it (e.g. "drieÃ«ndertig" contains both "drie" and "dertig"). Here we have to be carefull for overfitting.

\paragraph{SpaCy} SpaCy can appearantly also recognize functions, which is slightly worse than the previous aproach, but probably less vulnerable to overfitting.

\begin{table}[]
\begin{tabular}{lll}
\textbf{Method}                                    & \textbf{F1-score} & \textbf{Accuracy} \\
TF-IDF                                             & 48.57\%           & 84.21\% (192/228) \\
TF-IDF (1-6 chars)                                 & 52.17\%           & 85.53\% (195/228) \\
Simple Function (digit or number text in headline) & 80.00\%           & 90.35\% (206/228) \\
SpaCy (contains number)                            & 77.78\%           & 89.47\% (204/228)
\end{tabular}
\caption{Detecting label "Cijfers"}
\label{tab:numbers}
\end{table}

activity, length, questions, punctuation, duality, emotions, forward reference, signal words, article words, adjectives, proper names, directly adressing the reader, first and last names, numbers and quotations

\paragraph{Classification}
 In our use case, we want to predict for a given news headline whether it's the winner or not, so instead of actually predicting a class, we'll predict the probability the class winner would get assigned and give the candidate headline with the highest such probability the winner label for that test set.




\clearpage
\bibliographystyle{plain}
\bibliography{references}
\clearpage

-------------------------------------

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{distribution.png}
\caption{Distribution of number of sales per item}
\label{fig:distribution}
\end{figure}

\section{Code}
The main code can be found in assignment4.ipynb. There is also some code (mostly based on assignment 1) for generating the association rules in association\_rules/\_\_init\_\_.py.
\end{document}