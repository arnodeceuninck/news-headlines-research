{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vectorizer\n",
    "In order to classify our headlines into categories, we first need to convert our headlines into a vector representation to feed it to a classifier.\n",
    "\n",
    "## CountVectorizer\n",
    "A count vectorizer is one such way to convert a sentence into a vector.\n",
    "\n",
    "### Preprocessing\n",
    "The CountVectorizer (just like a lot of other vectorizers) starts with preprocessing the data, e.g. by removing capital letter, punctuation, etc.\n",
    "\n",
    "### Tokenization\n",
    "It also tokenizes the data, e.g. by splitting the sentence into words. This gives word-grams. Often those word grams are of length 1, but you can set an n-gram range to get n-grams of different lengths (e.g. (1, 3) will generate all tuples from 1 to 3 words). Besides word grams, you can also generate character n-grams (where you take the grams per letter/character instead of per word).\n",
    "\n",
    "### Vectorization\n",
    "The vectorization step is the last step in the CountVectorizer. It converts the tokenized data into a vector. Each position in this vector represents one of our token. The number in this position is how many times this token occurs in given sentence."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['arno', 'ben', 'bob', 'hallo', 'hey', 'ik', 'of'], dtype=object)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform([\"Hallo ik ben Arno\", \"Hey Arno\", \"Ik ben Bob of ben ik Bob?\"])\n",
    "\n",
    "count_vect.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 1, 0, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1, 0, 0],\n       [0, 2, 2, 0, 0, 2, 1]], dtype=int64)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TfidfVectorizer\n",
    "Tf-Idf stands for \"Term Frequency\"-\"Inverse Document frequency\". We already determined the \"Term Frequency\" for each of the terms (tokens) using our count vectorizer. To go from the token count to the tf-idf value, you divide it by the total number of times that token appears in any of the document. ([src](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) Both the counting and dividing is done by the TfidfVectorizer.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['arno', 'ben', 'bob', 'hallo', 'hey', 'ik', 'of'], dtype=object)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "train_counts = tfidf_vect.fit_transform([\"Hallo ik ben Arno\", \"Hey Arno\", \"Ik ben Bob of ben ik Bob?\"])\n",
    "\n",
    "\n",
    "tfidf_vect.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.45985353, 0.45985353, 0.        , 0.60465213, 0.        ,\n        0.45985353, 0.        ],\n       [0.60534851, 0.        , 0.        , 0.        , 0.79596054,\n        0.        , 0.        ],\n       [0.        , 0.4902234 , 0.64458485, 0.        , 0.        ,\n        0.4902234 , 0.32229243]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This gives us a vector representation for each of the headlines, which we can feed to our classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Informative features\n",
    "A vectorizer determines the weights for different words. The model adjusts some of its own weights while training to make predictions based on the weights of the vectorizer. In the util files, you can find a function `show_most_informative_features` which shows the most important words for a specific model/vectorizer.\n",
    "\n",
    "## Stemmer\n",
    "A stemmer converts similar words to the same stem. E.g. when you have the words \"run\", \"running\", \"ran\", \"runner\", \"runs\", the stemmer will convert them all to \"run\". This is useful because it reduces the number of features (tokens) in our vector representation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}